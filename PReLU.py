''''
PReLU (Parametric Rectified Linear Unit)

В этом примере функция prelu принимает входной массив x и параметр alpha, который является обучаемым коэффициентом активации. Функция сначала применяет ReLU к каждому элементу массива x с помощью np.maximum(0, x), а затем при неотрицательных значениях x просто оставляет его без изменений, а при отрицательных значениях умножает их на alpha с использованием np.minimum(0, x).

В приведенном примере мы передаем массив -2, 0, 4, -3, 1 и устанавливаем alpha равным 0.1. Результатом будет массив  -0.2, 0, 4, -0.3, 1, где каждый элемент модифицирован с помощью активации PReLU.

import numpy as np

def prelu(x, alpha):
    return np.maximum(0, x) + alpha * np.minimum(0, x)

# Пример использования
x = np.array([-2, 0, 4, -3, 1])
alpha = 0.1

output = prelu(x, alpha)
print(output)


В данном примере `alpha` является параметром активации PReLU. Он представляет собой числовое значение, которое контролирует насколько нижнюю ветку PReLU следует подавить в отрицательных значениях входного массива.

В активации PReLU функция ReLU применяется к каждому элементу массива входных данных. Однако, когда `x` отрицателен, для сдвига "левой" (отрицательной) ветки функции ReLU используется параметр `alpha`. Таким образом, если `x` положительное, то применяется обычная функция ReLU (x), а если `x` отрицательное, то применяется x * alpha.

В приведенном примере `alpha` установлено равным 0.1. Это означает, что положительные значения `x` будут оставлены без изменений, а отрицательные значения будут умножены на 0.1. Таким образом, "левая" ветка PReLU будет подавлена на 90% в сравнении с ее исходным значением.

Этот параметр `alpha` может быть настроен в зависимости от конкретной задачи и входных данных для достижения лучшей производительности модели.
'''
def prelu(x, alpha):
    return np.maximum(0, x) + alpha * np.minimum(0, x)